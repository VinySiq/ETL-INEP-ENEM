{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Vinicius Siqueira\n",
    "E-mail: viniciussiqueira.dev@outlook.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "Este notebook foi feito em um cluster AWS EMR utilizando PySpark como ferramenta para processamento de big data. É apenas um simples notebook que tem o intuito de visualizar o schema e alguns dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import mean, max, min, col, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "enem = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"delimiter\",\";\")\n",
    "    .load(\"s3://datalake-enem2020-vinisiq/data/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enem.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    enem\n",
    "    .select(\"NU_INSCRICAO\",\"NU_ANO\",\"TP_SEXO\",\"NO_MUNICIPIO_PROVA\",\n",
    "           \"SG_UF_PROVA\",\"NU_NOTA_CN\",\"NU_NOTA_CH\",\"NU_NOTA_LC\",\n",
    "           \"NU_NOTA_MT\",\"NU_NOTA_REDACAO\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .groupby(\"TP_SEXO\")\n",
    "    .agg(\n",
    "        mean(col(\"NU_ANO\")).alias(\"ANO_ENEM\"),\n",
    "        count(col(\"TP_SEXO\")).alias(\"CONTAGEM\"),\n",
    "        max(col(\"NU_NOTA_LC\")).alias(\"MAX_NOTA\"),\n",
    "        min(col(\"NU_NOTA_LC\")).alias(\"MIN_NOTA\"),\n",
    "    )\n",
    " .show(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformação para parquet realizada pelo arquivo job_spark.py via Steps do AWS EMR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caminho para código que converteu os dados de csv para parquet:\n",
    "functions > controllers > EMR > job_spark.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
